{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "from lxml import etree\n",
    "from spacy.en import English\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# magic to force own tokenization\n",
    "# https://github.com/explosion/spaCy/issues/182\n",
    "def my_split_function(string):\n",
    "    return string.split()\n",
    "\n",
    "old_tokenizer = nlp.tokenizer\n",
    "nlp.tokenizer = lambda string: old_tokenizer.tokens_from_list(my_split_function(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ecb_parser_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_tid_and_sent_dict(doc):\n",
    "    \"\"\"\n",
    "    given a loaded ECB(+) xml file, extract two dictionaries:\n",
    "    1. mapping t_id -> ecb_parser_classes.Token instance\n",
    "    2. mapping sentence identifier -> sentence (list of strings)\n",
    "    \n",
    "    :param lxml.etree._ElementTree doc: loaded ECB(+) xml file\n",
    "    \n",
    "    :rtype: tuple\n",
    "    :return: (tid2token_objs, sentence_dict)\n",
    "    \"\"\"\n",
    "    sentence_dict = defaultdict(list)\n",
    "    tid2token_objs = dict()\n",
    "    \n",
    "    for token_el in doc.iterfind('token'):\n",
    "        token_el_obj = ecb_parser_classes.Token(token_el)\n",
    "\n",
    "        tid2token_objs[token_el_obj.t_id] = token_el_obj\n",
    "        sentence_dict[token_el_obj.sentence].append(token_el_obj.token)\n",
    "    \n",
    "    return tid2token_objs, sentence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_mid_dict(doc):\n",
    "    \"\"\"\n",
    "    given a loaded ECB(+) xml file, extract one dictionary:\n",
    "    1. mapping m_id -> list of ecb_parser_classes.Token instances\n",
    "    \n",
    "    :param lxml.etree._ElementTree doc: loaded ECB(+) xml file\n",
    "    \n",
    "    :rtype: dict\n",
    "    :return: mid2tids\n",
    "    \"\"\"\n",
    "    mid2tids = dict()\n",
    "    \n",
    "    for event_mention_el in doc.iterfind('Markables/*[token_anchor]'):\n",
    "        m_id = event_mention_el.get('m_id')\n",
    "        t_ids = {int(token_anchor_el.get('t_id'))\n",
    "                 for token_anchor_el in event_mention_el.iterfind('token_anchor') }\n",
    "        mid2tids[m_id] = [tid2token_objs[t_id] for t_id in sorted(t_ids)]\n",
    "    \n",
    "    return mid2tids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_tokens_with_lemma(tid2token_objs, pos_mapping, debug=False):\n",
    "    \"\"\"\n",
    "    given the tid2token_objs dictionary (output from load_tid_and_sent_dict)\n",
    "    this function adds the lemma attribute to all ecb_parser_classes.Token instances\n",
    "    \n",
    "    :param dict tid2token_objs: mapping t_id -> ecb_parser_classes.Token instance\n",
    "    :param dict pos_mapping: mapping from spacy pos tagset to wordnet pos tagset\n",
    "    \"\"\"\n",
    "    text = ' '.join([token_el_obj.token\n",
    "                     for t_id, token_el_obj in sorted(tid2token_objs.items())])\n",
    "    parsed_text = nlp(text)\n",
    "    for t_id, token in enumerate(parsed_text, 1):\n",
    "        if debug:\n",
    "            print(t_id, token.lemma_, token.pos_)\n",
    "        tid2token_objs[t_id].set_lemma(token.lemma_)\n",
    "        mapped_pos = pos_mapping[token.pos_]\n",
    "        tid2token_objs[t_id].set_pos(mapped_pos)\n",
    "        \n",
    "    if debug:\n",
    "        input('continue?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_path_generator(ecb_folder, ecb=False, ecbplus=False):\n",
    "    \"\"\"\n",
    "    function to create an iterable over the ecb(+) corpus\n",
    "    \n",
    "    :param str ecb_folder: path to ecb_folder\n",
    "    :param bool ecb: if True, ecb files are included\n",
    "    :param bool ecbplus: if True, ecbplus files are included\n",
    "    \n",
    "    :rtype: generator\n",
    "    :return: (lxml.etree._ElementTree of xml file, document name, document identifier)\n",
    "    \"\"\"\n",
    "    if ecb and ecbplus:\n",
    "        glob_path = '/*.xml'\n",
    "    elif ecb and not ecbplus:\n",
    "        glob_path = '/*ecb.xml'\n",
    "    elif not ecb and ecbplus:\n",
    "        glob_path = '/*ecbplus.xml'\n",
    "        \n",
    "    for topic_number in range(1, 46):\n",
    "        folder_path = os.path.join(ecb_folder, str(topic_number))\n",
    "        for xml_path in glob(folder_path + glob_path):\n",
    "            doc = etree.parse(xml_path)\n",
    "            root = doc.getroot()\n",
    "            doc_name = root.get('doc_name')\n",
    "            doc_id = root.get('doc_id')\n",
    "            yield doc, doc_name, doc_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "include_ecb = True\n",
    "include_ecbplus = True\n",
    "path_ecb_folder = 'ECB+'\n",
    "\n",
    "pos_mapping = defaultdict(lambda: 'reste')\n",
    "pos_mapping['NOUN'] = 'n'\n",
    "pos_mapping['VERB'] = 'v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ev_instances = dict()\n",
    "ev_triggers = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for doc, doc_name, doc_id in create_path_generator(path_ecb_folder, \n",
    "                                                   ecb=include_ecb, \n",
    "                                                       ecbplus=include_ecbplus):\n",
    "    tid2token_objs, sentence_dict = load_tid_and_sent_dict(doc)\n",
    "    mid2tids = load_mid_dict(doc)\n",
    "    update_tokens_with_lemma(tid2token_objs, pos_mapping, debug=False)\n",
    "    \n",
    "    for an_event_instance_el in doc.iterfind('Markables/*[@instance_id]'):\n",
    "        event_instance_obj = ecb_parser_classes.EventInstance(event_instance_el=an_event_instance_el)\n",
    "        if 'ACTION' in event_instance_obj.xml_tag:\n",
    "            ev_instances[event_instance_obj.instance_id] = event_instance_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for doc, doc_name, doc_id in create_path_generator('ECB+', \n",
    "                                                   ecb=include_ecb, \n",
    "                                                   ecbplus=include_ecbplus):\n",
    "    \n",
    "    tid2token_objs, sentence_dict = load_tid_and_sent_dict(doc)\n",
    "    mid2tids = load_mid_dict(doc)\n",
    "    update_tokens_with_lemma(tid2token_objs, pos_mapping)\n",
    "    \n",
    "    for a_cross_doc_coref_el in doc.iterfind('Relations/CROSS_DOC_COREF'):\n",
    "        event_instance_id = a_cross_doc_coref_el.get('note')\n",
    "\n",
    "        for source_el in a_cross_doc_coref_el.iterfind('source'):\n",
    "            m_id = source_el.get('m_id')\n",
    "            token_objs = mid2tids[m_id]\n",
    "            sentence_number = token_objs[0].sentence\n",
    "            sentence = ' '.join(sentence_dict[sentence_number])\n",
    "\n",
    "            event_mention_obj = ecb_parser_classes.EventMention(event_instance_id,\n",
    "                                                                token_objs,\n",
    "                                                                doc_name,\n",
    "                                                                doc_id,\n",
    "                                                                m_id,\n",
    "                                                                sentence)\n",
    "\n",
    "            if event_instance_id in ev_instances:\n",
    "                event_instance_obj = ev_instances[event_instance_id]\n",
    "                event_instance_obj.event_mentions.add(event_mention_obj)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for event_instance_id, event_instance_obj in ev_instances.items():\n",
    "    for event_mention_obj in event_instance_obj.event_mentions:\n",
    "        trigger, pos = event_mention_obj.event_trigger\n",
    "        \n",
    "        if trigger not in ev_triggers:\n",
    "            event_trigger_obj = ecb_parser_classes.EventTrigger(event_trigger=trigger)\n",
    "            ev_triggers[trigger] = event_trigger_obj\n",
    "        \n",
    "        event_trigger_obj = ev_triggers[trigger]\n",
    "        event_trigger_obj.event_instances.add(event_instance_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('cache/ecbANDecbplus.bin' ,'wb') as outfile:\n",
    "    pickle.dump((ev_instances, ev_triggers), outfile)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
